{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "import urllib\n",
    "import urllib2\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "\n",
    "hdr = {\n",
    "    'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "    'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "    'Accept-Encoding': 'none',\n",
    "    'Accept-Language': 'en-US,en;q=0.8',\n",
    "    'Connection': 'keep-alive'\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def getUrlSoup(url,hdr):\n",
    "    req1 = urllib2.Request(url, headers=hdr)\n",
    "    page = urllib2.urlopen(req1)\n",
    "    content = page.read()\n",
    "    soup = BeautifulSoup(content)\n",
    "    return soup\n",
    "\n",
    "\n",
    "\n",
    "def getReviews(main_block, reviews_obj=None):\n",
    "    \n",
    "    assert reviews_obj is not None\n",
    "    \n",
    "    # Treat it as a massive block of text and split it on the horizontal lines.\n",
    "    reviews_block = re.split(r\"<hr[^>]*>\", str(main_block))\n",
    "    \n",
    "    # Iterate over all the reviews.\n",
    "    # NOTE: The indexing is dubious here...\n",
    "    for r in reviews_block[1:-3]:\n",
    "        \n",
    "        # Soup the block of html-code-text\n",
    "        soup_review = BeautifulSoup(r)\n",
    "        soup_review_header = soup_review.find('div')\n",
    "        \n",
    "        # Create an empty dictionary to hold the review.\n",
    "        review = dict()\n",
    "        \n",
    "        # Get the user-name, user-link for the reviewer.\n",
    "        review['user_name'] = soup_review_header.findAll('a')[1].get_text()\n",
    "        review['user_href'] = soup_review_header.findAll('a')[1]['href']\n",
    "\n",
    "        # Try to get the rating.\n",
    "        soup_imgs = soup_review_header.findAll('img')\n",
    "        review['review_rating'] = None\n",
    "        if len(soup_imgs) > 1:\n",
    "            review['review_rating'] = int(soup_imgs[1]['alt'].split('/')[0])\n",
    "        \n",
    "        # Get the review title.\n",
    "        review['review_title'] = soup_review.find('h2').get_text()\n",
    "        \n",
    "        # Get the review content.\n",
    "        tmp_content = soup_review.findAll('p')\n",
    "        if len(tmp_content) > 1 and 'spoilers' in tmp_content[0].get_text():\n",
    "            review['review_content'] = tmp_content[1].get_text()\n",
    "        else:\n",
    "            review['review_content'] = tmp_content[0].get_text()\n",
    "        \n",
    "        # Get the review usefulness.\n",
    "        # review['review_useful'] = soup_review_header.find('small').get_text().split('people')[0].rstrip().lstrip()\n",
    "        \n",
    "        reviews_obj.append(review)\n",
    "        \n",
    "    # Done. Return the reviews_obj\n",
    "    return reviews_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the list of movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 9869 movies in the dataset.\n"
     ]
    }
   ],
   "source": [
    "MOVIES = json.load(open('../../data-blog---list_of_movies_v2-v125.json','r'))\n",
    "\n",
    "print 'We have {} movies in the dataset.'.format(len(MOVIES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each movie we will grab the ID and get the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grabbing reviews for all movies...\n",
      "\n",
      "0 . 1 . 2 . 3 . 4 . 5 . 6 . 7 . 8 . 9 . 10 . 11 . 12 . 13 . 14 . 15 . 16 . 17 . 18 . 19 . 20 . 21 . 22 . 23 . 24 . 25 . 26 . 27 . 28 . 29 . 30 . 31 . 32 . 33 . 34 . 35 . 36 . 37 . 38 . 39 . 40 . 41 . 42 . 43 . 44 . 45 . 46 . 47 . 48 . 49 . 50 . 51 . 52 . 53 . 54 . 55 . 56 . 57 . 58 . 59 . 60 . 61 . 62 . 63 . 64 . 65 . 66 . 67 . 68 . 69 . 70 . 71 . 72 . 73 . 74 . 75 . 76 . 77 . 78 . 79 . 80 . 81 . 82 . 83 . 84 . 85 . 86 . 87 . 88 . 89 . 90 . 91 . 92 . 93 . 94 . 95 . 96 . 97 . 98 . 99 . 100 . 101 . 102 . 103 . 104 . 105 . 106 . 107 . 108 . 109 . 110 . 111 . 112 . 113 . 114 . 115 . 116 . 117 . 118 . 119 . 120 . 121 . 122 . 123 . 124 . \t --->  The Sixth Sense  -- pages:  208\n",
      "125 . \t --->  Toy Story 2  -- pages:  51\n",
      "126 . \t --->  American Psycho  -- pages:  105\n",
      "127 . \t --->  Scary Movie  -- pages:  89\n",
      "128 . \t --->  Snatch.  -- pages:  73\n",
      "129 . DONE! Added 5245 reviews\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luuk/pyEnv/lib/python2.7/site-packages/bs4/__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"html.parser\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "print 'Grabbing reviews for all movies...\\n'\n",
    "\n",
    "added = 0\n",
    "\n",
    "# Iterate over all movies.\n",
    "for e,movie in enumerate(MOVIES[:130]):\n",
    "    \n",
    "    # Check if the movie already has the reviews populated\n",
    "    if 'reviews' not in movie:\n",
    "        \n",
    "        print '\\t ---> ', movie['name'],\n",
    "    \n",
    "        url = 'http://www.imdb.com' + movie['link'] + 'reviews'\n",
    "\n",
    "        # Get the contents for the first reviews page.\n",
    "        soup = getUrlSoup(url,hdr)\n",
    "        # Get the main content block.\n",
    "        main_block = (soup.body.find('div', attrs={'class':'reviews'})\n",
    "                               .find('div', attrs={'id':'tn15main'})\n",
    "                               .find('div', attrs={'id':'tn15content'}))\n",
    "        # Get the number of pages to get reviews from.\n",
    "        num_pages = int(main_block.findAll('table')[1]\n",
    "                                  .find('td',attrs={'nowrap':'1'}).get_text().split(' of ')[1].rstrip(':'))\n",
    "\n",
    "        MOVIES[e]['num_reviews_pages'] = num_pages\n",
    "        print ' -- pages: ', num_pages\n",
    "\n",
    "        # Iterate over all the reviews pages.\n",
    "        REVIEWS = []\n",
    "\n",
    "        for i in range(1,num_pages+1):\n",
    "\n",
    "            # If this is the first page, we don't need to load the new page.\n",
    "            if i == 1:\n",
    "                REVIEWS = getReviews(main_block,REVIEWS)\n",
    "\n",
    "            # We need to grab the contents for the next page.\n",
    "            else:\n",
    "                page_soup = getUrlSoup(url + '?start=' + str(10*(i-1)), hdr)\n",
    "                main_block = (page_soup.body.find('div', attrs={'class':'reviews'})\n",
    "                                            .find('div', attrs={'id':'tn15main'})\n",
    "                                            .find('div', attrs={'id':'tn15content'}))\n",
    "                REVIEWS = getReviews(main_block,REVIEWS)\n",
    "\n",
    "        # Done for this movie. Add reviews to the movie object.\n",
    "        MOVIES[e]['reviews'] = REVIEWS\n",
    "        added += len(REVIEWS)\n",
    "\n",
    "\n",
    "    # If the review is there. Sanity check...\n",
    "    # else:\n",
    "#         # nothing.\n",
    "#         url = 'http://www.imdb.com' + movie['link'] + 'reviews'\n",
    "#         soup = getUrlSoup(url,hdr)\n",
    "#         # Get the main content block.\n",
    "#         main_block = (soup.body.find('div', attrs={'class':'reviews'})\n",
    "#                                .find('div', attrs={'id':'tn15main'})\n",
    "#                                .find('div', attrs={'id':'tn15content'}))\n",
    "#         # Get the number of pages to get reviews from.\n",
    "#         num_pages = int(main_block.findAll('table')[1]\n",
    "#                                   .find('td',attrs={'nowrap':'1'}).get_text().split(' of ')[1].rstrip(':'))\n",
    "#         if \n",
    "        \n",
    "    if e % 1 == 0:\n",
    "        print e, '.',\n",
    "        \n",
    "\n",
    "print 'DONE! Added {} reviews'.format(added)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147562\n"
     ]
    }
   ],
   "source": [
    "s = 0\n",
    "for m in MOVIES:\n",
    "    if 'reviews' in m:\n",
    "        # print m['name'], len(m['reviews'])\n",
    "        s += len(m['reviews'])\n",
    "    else:\n",
    "        break\n",
    "        \n",
    "print s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Full dump\n",
    "with open('../../data-blog---list_of_movies_v2-v130.json', 'w') as outfile:\n",
    "    json.dump(MOVIES, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Dump JSONL line by line\n",
    "# with open('../../list_of_movies_with_reviews.json', 'a') as the_file:\n",
    "# the_file.write('Hello\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
