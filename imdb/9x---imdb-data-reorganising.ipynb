{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "# --------------------------------------------- #\n",
    "def fnShowNumberOfReviews(MOVIES):\n",
    "    s = 0\n",
    "    for m in MOVIES:\n",
    "        s += len(m['reviews']) if 'reviews' in m else 0\n",
    "    return s\n",
    "    # --------------------------------------------- #\n",
    "\n",
    "\n",
    "DATA_PATH = '/Users/luuk/DATA/IMDB/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the list of movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 9869 movies in the dataset.\n",
      "Number of reviews available: 648988\n"
     ]
    }
   ],
   "source": [
    "MOVIES = json.load(open(DATA_PATH + 'data-blog---MASTER-v1313.json','r'))\n",
    "\n",
    "print 'We have {} movies in the dataset.'.format(len(MOVIES))\n",
    "print 'Number of reviews available: {}'.format(fnShowNumberOfReviews(MOVIES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to take the first 250 movies and put them in a seperate `jsonl` file. Each movie will occupy one line of the file.\n",
    "<br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Wrote the movies to data-blog---imdb-movies-5-1000-1249.jsonl\n"
     ]
    }
   ],
   "source": [
    "ix = 5\n",
    "\n",
    "fromIx = 250 * (ix-1)\n",
    "toIx = 250 * ix - 1\n",
    "\n",
    "\n",
    "FILENAME_OUTPUT = 'data-blog---imdb-movies-' + str(ix) + '-' + str(fromIx) + '-' + str(toIx) + '.jsonl'\n",
    "\n",
    "with open(DATA_PATH + FILENAME_OUTPUT, 'w+') as f:\n",
    "    for movie in MOVIES[fromIx:(toIx+1)]:\n",
    "        f.write(json.dumps(movie) + os.linesep)\n",
    "\n",
    "print 'Done! Wrote the movies to {}'.format(FILENAME_OUTPUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test.\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 250 movies in the dataset.\n",
      "Number of reviews available: 74170\n"
     ]
    }
   ],
   "source": [
    "MOVIES = None\n",
    "MOVIES = []\n",
    "\n",
    "\n",
    "with open(DATA_PATH + FILENAME_OUTPUT, 'r') as f:\n",
    "    for movie in f.readlines():\n",
    "        MOVIES.append(json.loads(movie))\n",
    "\n",
    "\n",
    "print 'We have {} movies in the dataset.'.format(len(MOVIES))\n",
    "print 'Number of reviews available: {}'.format(fnShowNumberOfReviews(MOVIES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert this data to **ElasticSearch** we need to add a line before each movie to create the appropriate create-entry for the **ElasticSearch** Bulk API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MOVIES = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done converting data-blog---imdb-movies-5-1000-1249.jsonl to ElasticSearch Bulk API format!\n"
     ]
    }
   ],
   "source": [
    "READ_FILE = FILENAME_OUTPUT\n",
    "WRITE_FILE = FILENAME_OUTPUT + '-ElasticSearch.jsonl'\n",
    "\n",
    "\n",
    "# Open the file you want to read from\n",
    "with open(DATA_PATH + READ_FILE,'r') as read_from_file:\n",
    "    \n",
    "    # Open the file you want to write to\n",
    "    with open(DATA_PATH + WRITE_FILE, 'w+') as write_to_file:\n",
    "        \n",
    "        # Read each line from the read-file.\n",
    "        for json_line in read_from_file.readlines():\n",
    "            \n",
    "            # Write it to the write-file. Always write an ElasticSearch create line first.\n",
    "            write_to_file.write('{ \"index\":  {}}\\n')\n",
    "            write_to_file.write(json.dumps(json.loads(json_line)) + '\\n')\n",
    "\n",
    "\n",
    "print 'Done converting {} to ElasticSearch Bulk API format!'.format(READ_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
